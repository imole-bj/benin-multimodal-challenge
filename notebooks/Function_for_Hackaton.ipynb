{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfPjqn4tUQWc"
      },
      "outputs": [],
      "source": [
        "!pip install ffmpeg-python\n",
        "!pip install pydub\n",
        "!pip install transformers\n",
        "!pip install srt\n",
        "!pip install -U openai-whisper\n",
        "!pip install gTTS\n",
        "!pip install --upgrade transformers accelerate\n",
        "!pip install openai==1.30.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AcYw_JYUqao"
      },
      "outputs": [],
      "source": [
        "!sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COVPF3EIUkK4"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import subprocess\n",
        "import ffmpeg\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import detect_nonsilent\n",
        "from transformers import AutoProcessor, WhisperForConditionalGeneration\n",
        "import whisper\n",
        "import os\n",
        "from datetime import timedelta\n",
        "import re\n",
        "import srt\n",
        "from transformers import VitsModel, AutoTokenizer\n",
        "import torch\n",
        "import scipy\n",
        "import torch\n",
        "import numpy as np\n",
        "from gtts import gTTS\n",
        "import tempfile\n",
        "import sys\n",
        "import json\n",
        "import datetime\n",
        "import ffmpeg\n",
        "from moviepy.editor import VideoFileClip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDI-59X0Ulh4"
      },
      "outputs": [],
      "source": [
        "#fonction 1 pour extraire l'audio de video mp4 et le convertir en .wav\n",
        "def convert_to_wav(input_file_path):\n",
        "  \"\"\"\n",
        "  Converts an audio file to WAV format.\n",
        "\n",
        "  Args:\n",
        "      input_file (str): Path to the input audio file.\n",
        "      output_file (str): Path to the output WAV file.\n",
        "  \"\"\"\n",
        "  if isinstance(input_file_path, str):\n",
        "      output_file_path = input_file_path.split('.')[0].lower() + \"_audio.wav\"\n",
        "\n",
        "  try:\n",
        "      result = subprocess.run(\n",
        "          [\"ffmpeg\", \"-y\", \"-i\", input_file_path, \"-acodec\", \"pcm_s16le\", output_file_path],\n",
        "          capture_output=True,  # Capture la sortie standard et la sortie d'erreur\n",
        "          text=True           # Décode la sortie en texte\n",
        "      )\n",
        "  except subprocess.CalledProcessError as e:\n",
        "      print(f\"Erreur ffmpeg: {e}\")\n",
        "      print(f\"Sortie standard: {e.stdout}\")\n",
        "      print(f\"Sortie d'erreur: {e.stderr}\")\n",
        "  else:\n",
        "      print(\"Conversion réussie!\")\n",
        "\n",
        "\n",
        "  return output_file_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #fonction 2  pour supprimer le silence dans l'audio\n",
        "def remove_silence_from_end(input_file_path, silence_threshold=-50.0, chunk_size=10, is_start=True):\n",
        "    \"\"\"\n",
        "    Removes silence from the end of an audio file.\n",
        "\n",
        "    :param input_file_path: path to the input mp3 file\n",
        "    :param silence_threshold: the threshold in dBFS considered as silence\n",
        "    :param chunk_size: the chunk size to use in silence detection (in milliseconds)\n",
        "    :return: path to the output mp3 file without silence at the end\n",
        "    \"\"\"\n",
        "    # Load the audio file\n",
        "    format = \"wav\"\n",
        "    if isinstance(input_file_path, str):\n",
        "        format = input_file_path.split('.')[-1].lower()\n",
        "        if format in ['wav', 'mp3','m4a']:\n",
        "            audio = AudioSegment.from_file(input_file_path, format=format if format in ['wav','mp3'] else 'mp4')\n",
        "        else:\n",
        "            # Convert to mp3\n",
        "            try:\n",
        "                audio.export(input_file_path + \".mp3\", format=\"mp3\")\n",
        "                input_file_path = input_file_path + \".mp3\"\n",
        "            except Exception as e:\n",
        "                print(\"Error:\", e)\n",
        "                return input_file_path\n",
        "\n",
        "    else:\n",
        "        audio = input_file_path\n",
        "\n",
        "    # Detect non-silent chunks\n",
        "    nonsilent_chunks = detect_nonsilent(\n",
        "        audio,\n",
        "        min_silence_len=chunk_size,\n",
        "        silence_thresh=silence_threshold\n",
        "    )\n",
        "\n",
        "    # If we have nonsilent chunks, get the start and end of the last nonsilent chunk\n",
        "    if nonsilent_chunks:\n",
        "        start_index, end_index = nonsilent_chunks[-1]\n",
        "    else:\n",
        "        # If the whole audio is silent, just return it as is\n",
        "        return input_file_path\n",
        "\n",
        "    # Remove the silence from the end by slicing the audio segment\n",
        "    trimmed_audio = audio[:end_index]\n",
        "    if is_start and nonsilent_chunks[0] and nonsilent_chunks[0][0] > 0:\n",
        "        trimmed_audio = audio[nonsilent_chunks[0][0]:end_index]\n",
        "\n",
        "    # Export the trimmed audio to mp3 format using ffmpeg-python\n",
        "    output_file_path = input_file_path\n",
        "    if isinstance(input_file_path, str):\n",
        "        output_file_path = input_file_path.split('.')[0].lower() + \"_trimmed.wav\"\n",
        "        (\n",
        "            ffmpeg\n",
        "            .input(input_file_path)\n",
        "            .output(output_file_path, format='wav')\n",
        "            .run()\n",
        "        )\n",
        "    return output_file_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #fonction mapping dict\n",
        "def rename_keys(s_dict):\n",
        "\n",
        "    WHISPER_MAPPING = {\n",
        "    \"encoder.ln_post.weight\": \"encoder.layer_norm.weight\", # added by ax\n",
        "    \"encoder.ln_post.bias\": \"encoder.layer_norm.bias\", # added by ax\n",
        "    \"blocks\": \"layers\",\n",
        "    \"mlp.0\": \"fc1\",\n",
        "    \"mlp.2\": \"fc2\",\n",
        "    \"mlp_ln\": \"final_layer_norm\",\n",
        "    \".attn.query\": \".self_attn.q_proj\",\n",
        "    \".attn.key\": \".self_attn.k_proj\",\n",
        "    \".attn.value\": \".self_attn.v_proj\",\n",
        "    \".attn_ln\": \".self_attn_layer_norm\",\n",
        "    \".attn.out\": \".self_attn.out_proj\",\n",
        "    \".cross_attn.query\": \".encoder_attn.q_proj\",\n",
        "    \".cross_attn.key\": \".encoder_attn.k_proj\",\n",
        "    \".cross_attn.value\": \".encoder_attn.v_proj\",\n",
        "    \".cross_attn_ln\": \".encoder_attn_layer_norm\",\n",
        "    \".cross_attn.out\": \".encoder_attn.out_proj\",\n",
        "    \"decoder.ln.\": \"decoder.layer_norm.\",\n",
        "    \"encoder.ln.\": \"encoder.layer_norm.\",\n",
        "    \"token_embedding\": \"embed_tokens\",\n",
        "    \"encoder.positional_embedding\": \"encoder.embed_positions.weight\",\n",
        "    \"decoder.positional_embedding\": \"decoder.embed_positions.weight\",\n",
        "    #\"ln_post\": \"layer_norm\", # disabled by ax\n",
        "   }\n",
        "\n",
        "    keys = list(s_dict.keys())\n",
        "    for key in keys:\n",
        "        new_key = key\n",
        "        for v, k in WHISPER_MAPPING.items():\n",
        "            if k in key:\n",
        "                new_key = new_key.replace(k, v)\n",
        "\n",
        "        print(f\"{key} -> {new_key}\")\n",
        "\n",
        "        s_dict[new_key] = s_dict.pop(key)\n",
        "    return s_dict\n",
        "\n",
        "\n",
        "\n",
        "#fonction transcribe yoruba\n",
        "def transcription_yoruba(input_file_path, lang):\n",
        "    \"\"\"\n",
        "    Transcribe a video using Whisper.\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to the video (WAV file).\n",
        "    return:\n",
        "        result (dict): Dictionary containing the transcription results.\n",
        "        complet_chain (str): A string containing the completed transcription chain.\n",
        "    \"\"\"\n",
        "    #model.transcribe(\"/content/ren_frr.mp4\", language=\"fr\", verbose=False)\n",
        "\n",
        "    model = whisper.load_model(\"small\")\n",
        "\n",
        "    if lang == 'yo':\n",
        "\n",
        "      finetune_model = WhisperForConditionalGeneration.from_pretrained(\"steja/whisper-small-yoruba\")\n",
        "      state_dict = finetune_model.model.state_dict()\n",
        "      rename_keys(state_dict)\n",
        "      #model = whisper.load_model(\"small\")\n",
        "      missing, unexpected = model.load_state_dict(state_dict, strict = False)\n",
        "      if len(missing):\n",
        "          print(\"Weight name not found\", missing)\n",
        "          raise\n",
        "\n",
        "      result = model.transcribe(input_file_path, verbose=False)\n",
        "\n",
        "    elif lang == 'fr':\n",
        "       result=model.transcribe(input_file_path, language=\"fr\", verbose=False)\n",
        "\n",
        "    elif lang == 'en':\n",
        "       result=model.transcribe(input_file_path, language=\"en\", verbose=False)\n",
        "\n",
        "    complet_chain = \"\\n\".join(s['text'] for s in result[\"segments\"])\n",
        "\n",
        "    return result, complet_chain\n",
        "\n",
        "\n",
        "\n",
        "def generate_prompt_phrase(lang_source, lang_target, doc):\n",
        "\n",
        "    Prompt =    f'''\n",
        "\n",
        "    Traduit moi chacune des phrases du {lang_source} vers le {lang_target} :\n",
        "    {doc}\n",
        "    renvoie uniquement les phrases traduite et numérote les.\n",
        "    exemple: 1. phrase1\n",
        "             2. phrase2\n",
        "   '''\n",
        "\n",
        "    return Prompt\n",
        "\n",
        "\n",
        "\n",
        "def generate_prompt_text(lang_source, lang_target, texte):\n",
        "\n",
        "    Prompt =    f'''\n",
        "\n",
        "    Traduit moi cet texte du {lang_source} vers le {lang_target} :\n",
        "    {texte}\n",
        "    renvoie uniquement le texte traduit.\n",
        "\n",
        "   '''\n",
        "\n",
        "    return Prompt\n",
        "\n",
        "\n",
        "def get_gpt4_json_response(prompt):\n",
        "\n",
        "    #api_key_i=\"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\"\n",
        "    api_key_i = os.getenv('OPENAI_API_KEY')\n",
        "    client = OpenAI(\n",
        "\n",
        "    api_key=api_key_i,\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "        #response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def translate_gpt(lang_source, lang_target, doc):\n",
        "\n",
        "  dico_lang = dict([('fr', 'français'), ('yo', 'yoruba'), ('en', 'anglais'), ('den', 'dendi'), ('fon', 'fongbe')])\n",
        "  lang_source = dico_lang[lang_source]\n",
        "  lang_target = dico_lang[lang_target]\n",
        "\n",
        "  prompt = generate_prompt_text(lang_source, lang_target, doc)\n",
        "  response = get_gpt4_json_response(prompt)\n",
        "\n",
        "\n",
        "  return response\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def translate_gpt_phrase(lang_source, lang_target, doc):\n",
        "\n",
        "  dico_lang = dict([('fr', 'français'), ('yo', 'yoruba'), ('en', 'anglais'), ('den', 'dendi')])\n",
        "  lang_source = dico_lang[lang_source]\n",
        "  lang_target = dico_lang[lang_target]\n",
        "\n",
        "  prompt = generate_prompt_phrase(lang_source, lang_target, doc)\n",
        "  response = get_gpt4_json_response(prompt)\n",
        "\n",
        "  response_modif = re.sub(r'\\d+\\.', '', response)\n",
        "  liste_phrases =  response_modif.split('\\n')\n",
        "  return liste_phrases\n",
        "\n",
        "\n",
        "\n",
        "def ms_to_time_string(*, ms=0, seconds=None):\n",
        "    # Calculate hours, minutes, seconds and milliseconds\n",
        "    if seconds is None:\n",
        "        td = timedelta(milliseconds=ms)\n",
        "    else:\n",
        "        td = timedelta(seconds=seconds)\n",
        "    hours, remainder = divmod(td.seconds, 3600)\n",
        "    minutes, seconds = divmod(remainder, 60)\n",
        "    milliseconds = td.microseconds // 1000\n",
        "\n",
        "    time_string = f\"{hours}:{minutes}:{seconds},{milliseconds}\"\n",
        "    return format_time(time_string, ',')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def format_time(s_time=\"\", separate=','):\n",
        "    if not s_time.strip():\n",
        "        return f'00:00:00{separate}000'\n",
        "    s_time = s_time.strip()\n",
        "    hou, min, sec = \"00\", \"00\", f\"00{separate}000\"\n",
        "    tmp = s_time.split(':')\n",
        "    if len(tmp) >= 3:\n",
        "        hou = tmp[-3].strip()\n",
        "        min = tmp[-2].strip()\n",
        "        sec = tmp[-1].strip()\n",
        "    elif len(tmp) == 2:\n",
        "        min = tmp[0].strip()\n",
        "        sec = tmp[1].strip()\n",
        "    elif len(tmp) == 1:\n",
        "        sec = tmp[0].strip()\n",
        "\n",
        "    if re.search(r',|\\.', str(sec)):\n",
        "        sec, ms = re.split(r',|\\.', str(sec))\n",
        "        sec = sec.strip()\n",
        "        ms = ms.strip()\n",
        "    else:\n",
        "        ms = '000'\n",
        "    hou = hou if hou != \"\" else \"00\"\n",
        "    if len(hou) < 2:\n",
        "        hou = f'0{hou}'\n",
        "    hou = hou[-2:]\n",
        "\n",
        "    min = min if min != \"\" else \"00\"\n",
        "    if len(min) < 2:\n",
        "        min = f'0{min}'\n",
        "    min = min[-2:]\n",
        "\n",
        "    sec = sec if sec != \"\" else \"00\"\n",
        "    if len(sec) < 2:\n",
        "        sec = f'0{sec}'\n",
        "    sec = sec[-2:]\n",
        "\n",
        "    ms_len = len(ms)\n",
        "    if ms_len < 3:\n",
        "        for i in range(3 - ms_len):\n",
        "            ms = f'0{ms}'\n",
        "    ms = ms[-3:]\n",
        "    return f\"{hou}:{min}:{sec}{separate}{ms}\"\n",
        "\n",
        "\n",
        "\n",
        "# Ecrire les sous titre dans un fichier\n",
        "def generate_subtitles_raw(result, liste_phrases):\n",
        "\n",
        "  \"\"\"\n",
        "  return -->  [{'line': 1,\n",
        "    'time': '00:00:00,000 --> 00:00:15,320',\n",
        "    'text': 'Protégez-vous contre le COVID-19'},\n",
        "  {'line': 2,\n",
        "    'time': '00:00:15,440 --> 00:00:20,320',\n",
        "    'text': 'Renvoyez votre message ci-dessous sur le COVID-19'}\n",
        "    ]\n",
        "  \"\"\"\n",
        "  raw_subtitles = []\n",
        "  sidx = -1\n",
        "  #liste_phrases = tt_modifie.split('\\n')\n",
        "  for s, i in zip(result[\"segments\"], liste_phrases):\n",
        "    # for segment in result[\"segments\"]\n",
        "\n",
        "\n",
        "\n",
        "      sidx += 1\n",
        "      #start = int(segment.words[0].start * 1000)\n",
        "      #end = int(segment.words[-1].end * 1000)\n",
        "      start = s['start']* 1000\n",
        "      end = s['end']* 1000\n",
        "      # if start == end:\n",
        "      #     end += 200\n",
        "      startTime = ms_to_time_string(ms=start)\n",
        "      endTime = ms_to_time_string(ms=end)\n",
        "      text = i.strip().replace('&#39;', \"'\")\n",
        "      text = re.sub(r'&#\\d+;', '', text)\n",
        "      # No valid characters\n",
        "      if not text or re.match(r'^[，。、？‘’“”；：（｛｝【】）:;\"\\'\\s \\d`!@#$%^&*()_+=.,?/\\\\-]*$', text) or len(text) <= 1:\n",
        "          continue\n",
        "      # Original language subtitles\n",
        "      s = {\"line\": len(raw_subtitles) + 1, \"time\": f\"{startTime} --> {endTime}\", \"text\": text}\n",
        "      raw_subtitles.append(s)\n",
        "\n",
        "      return raw_subtitles\n",
        "\n",
        "\n",
        "\n",
        "#inserer les sous titre dans la video original\n",
        "def integrer_sous_titres(source_mp4_path, sous_nom, target_mp4_path, langue_sous_titres):\n",
        "  \"\"\"\n",
        "  Intègre un fichier de sous-titres dans une vidéo MP4 en utilisant ffmpeg.\n",
        "\n",
        "  Args:\n",
        "      source_mp4: Chemin vers le fichier vidéo MP4 source.\n",
        "      sous_nom: Chemin vers le fichier de sous-titres.\n",
        "      target_mp4: Chemin vers le fichier MP4 de sortie.\n",
        "      langue_sous_titres: Code de langue des sous-titres (ex: \"fra\").\n",
        "  \"\"\"\n",
        "\n",
        "  if isinstance(source_mp4_path, str):\n",
        "       target_mp4_path = source_mp4_path.split('.')[0].lower() + \"_video_avec_sous_titres.mp4\"\n",
        "\n",
        "  commande = [\n",
        "    \"ffmpeg\",\n",
        "    \"-y\",\n",
        "    \"-i\", source_mp4_path,\n",
        "    \"-i\", sous_nom,\n",
        "    \"-c\", \"copy\",\n",
        "    \"-c:s\", \"mov_text\",\n",
        "    \"-metadata:s:s:0\", f\"langue={langue_sous_titres}\",\n",
        "    target_mp4_path\n",
        "  ]\n",
        "\n",
        "  subprocess.run(commande)\n",
        "  #target_mp4_path = source_mp4_path\n",
        "\n",
        "  return target_mp4_path\n",
        "\n",
        "\n",
        "\n",
        "def generate_sound_yoruba(text):\n",
        "  model = VitsModel.from_pretrained(\"facebook/mms-tts-yor\")\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"facebook/mms-tts-yor\")\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "  with torch.no_grad():\n",
        "      output = model(**inputs).waveform\n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "\n",
        "def generate_audio_segment(result, liste_phrases, langa):\n",
        "    sidx = 0\n",
        "    segments = []\n",
        "    start_times = []\n",
        "\n",
        "    for s, i in zip(result[\"segments\"], liste_phrases):\n",
        "        sidx += 1\n",
        "        start = s['start'] * 1000\n",
        "        end = s['end'] * 1000\n",
        "        startTime = ms_to_time_string(ms=start)\n",
        "        endTime = ms_to_time_string(ms=end)\n",
        "\n",
        "        if langa == 'fr' or langa == 'en':\n",
        "            myobj = gTTS(text=i, lang=langa, slow=False)\n",
        "            with tempfile.NamedTemporaryFile(suffix=\".mp3\", delete=False) as tmp:\n",
        "                tmpname = tmp.name\n",
        "                myobj.save(tmpname)\n",
        "                audio_data = AudioSegment.from_file(tmpname, format=\"mp3\")\n",
        "        elif langa == 'yo':\n",
        "            myobj = generate_sound_yoruba(i)  # Supposons que cela retourne un objet numpy\n",
        "            myobj_np = myobj.numpy()  # Correction ici : Assurez-vous que myobj_np est correctement assigné\n",
        "            myobj_np = myobj_np.T\n",
        "            myobj_np = (myobj_np * 32767).astype(np.int16)\n",
        "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp:\n",
        "                tmpname = tmp.name\n",
        "                scipy.io.wavfile.write(tmpname, rate=16000, data=myobj_np)\n",
        "                sound = AudioSegment.from_wav(tmpname)\n",
        "                sound.export(tmpname, format=\"mp3\")\n",
        "                audio_data = AudioSegment.from_file(tmpname, format=\"mp3\")\n",
        "\n",
        "        segments.append(audio_data)\n",
        "        start_times.append(startTime)\n",
        "\n",
        "    return segments, start_times\n",
        "\n",
        "\n",
        "def runffprobe(cmd):\n",
        "    # cmd[-1] = os.path.normpath(cmd[-1])\n",
        "    try:\n",
        "        p = subprocess.run( cmd if isinstance(cmd,str) else ['ffprobe'] + cmd,\n",
        "                           stdout=subprocess.PIPE,\n",
        "                           stderr=subprocess.PIPE,\n",
        "                           encoding=\"utf-8\",\n",
        "                           text=True,\n",
        "                           check=True,\n",
        "                           creationflags=0 if sys.platform != 'win32' else subprocess.CREATE_NO_WINDOW)\n",
        "        if p.stdout:\n",
        "            return p.stdout.strip()\n",
        "        raise Exception(str(p.stderr))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        msg = f'ffprobe error:{str(e.stdout)},{str(e.stderr)}'\n",
        "        msg = msg.replace('\\n', ' ')\n",
        "        raise Exception(msg)\n",
        "    except Exception as e:\n",
        "        raise Exception(f'ffprobe except:{str(e)}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def convert_to_milliseconds(time_str):\n",
        "    \"\"\"Convertit une chaîne de caractères \"HH:MM:SS,mmm\" en millisecondes.\"\"\"\n",
        "    time_obj = datetime.datetime.strptime(time_str, \"%H:%M:%S,%f\")\n",
        "    milliseconds = int((time_obj.hour * 3600 + time_obj.minute * 60 + time_obj.second) * 1000 + time_obj.microsecond / 1000)\n",
        "    return milliseconds\n",
        "\n",
        "# Merge audio segments\n",
        "def merge_audio_segments(segments, start_times, total_duration, mp4name):\n",
        "    # Create an empty audio segment as the initial segment\n",
        "    merged_audio = AudioSegment.empty()\n",
        "    start_times_ms = [convert_to_milliseconds(time_str) if isinstance(time_str, str) else time_str for time_str in start_times]\n",
        "    print(start_times_ms )\n",
        "    # Check if silence needs to be added before the first segment\n",
        "    if start_times_ms[0]!= 0:\n",
        "        silence_duration = start_times_ms[0]\n",
        "        silence = AudioSegment.silent(duration=silence_duration)\n",
        "        merged_audio += silence\n",
        "\n",
        "    # Concatenate audio segments one by one\n",
        "    for i in range(len(segments)):\n",
        "        segment = segments[i]\n",
        "        start_time = start_times_ms[i]\n",
        "\n",
        "        # Check if there is a gap between the end time of the previous segment and the start time of the current segment\n",
        "        if i > 0:\n",
        "            previous_end_time = start_times_ms[i - 1] + segments[i - 1].duration_seconds * 1000\n",
        "            silence_duration = start_times_ms[i] - previous_end_time\n",
        "\n",
        "            # There may be a mismatch between subtitles and voice\n",
        "            if silence_duration > 0:\n",
        "                silence = AudioSegment.silent(duration=silence_duration)\n",
        "                merged_audio += silence\n",
        "            print(silence_duration)\n",
        "\n",
        "        # Concatenate the current segment\n",
        "        merged_audio += segment\n",
        "\n",
        "    # Check if the total duration exceeds the specified duration and discard the excess part\n",
        "    if len(merged_audio) > total_duration:\n",
        "        merged_audio = merged_audio[:total_duration]\n",
        "\n",
        "    # Export the merged audio\n",
        "    merged_audio.export(f\"./tmp/{mp4name}.wav\", format=\"wav\")\n",
        "\n",
        "    #return merged_audio\n",
        "    return f\"./tmp/{mp4name}.wav\"\n",
        "\n",
        "\n",
        "\n",
        "def delete_voice_video(input_file_path):\n",
        "    \"\"\"\n",
        "    Supprime la piste audio d'une vidéo et enregistre la vidéo sans son.\n",
        "\n",
        "    Args:\n",
        "        input_file_path (str): Chemin d'accès au fichier vidéo d'entrée.\n",
        "\n",
        "    Returns:\n",
        "        str: Chemin d'accès au fichier vidéo de sortie sans son.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(input_file_path, str):\n",
        "        output_file_path = input_file_path.split('.')[0].lower() + \"_novoice.mp4\"\n",
        "\n",
        "    # Construire la commande FFmpeg\n",
        "    ffmpeg_command = [\n",
        "        'ffmpeg',\n",
        "        '-y',\n",
        "        '-i', input_file_path,\n",
        "        '-map', '0:v',  # Copier le flux vidéo\n",
        "        '-c:v', 'copy',  # Copier le flux vidéo sans recodage\n",
        "        '-an',  # Supprimer la piste audio\n",
        "        output_file_path\n",
        "    ]\n",
        "\n",
        "    # Exécuter la commande FFmpeg\n",
        "    process = subprocess.run(ffmpeg_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "    # Afficher la sortie standard et la sortie d'erreur\n",
        "    out = process.stdout\n",
        "    err = process.stderr\n",
        "\n",
        "    # Vérifier les erreurs\n",
        "    if err:\n",
        "        print(f\"Erreur FFmpeg: {err.decode()}\")\n",
        "    else:\n",
        "        print(\"Vidéo créée avec succès\")\n",
        "\n",
        "    return output_file_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def merge_audio_video(video_file_path, audio_file_path):\n",
        "\n",
        "    \"\"\"\n",
        "      ex:\n",
        "      video_file = '/content/novoice.mp4'\n",
        "      audio_file = '/content/tmp/ren_merge_audio.wav'\n",
        "      output_file = '/content/voiceandvideo_1.mp4'\n",
        "    \"\"\"\n",
        "    if isinstance(video_file_path, str):\n",
        "      output_file_path = video_file_path.split('.')[0].lower() + \"voice_and_video.mp4\"\n",
        "\n",
        "    # Construire la commande FFmpeg\n",
        "    ffmpeg_command = [\n",
        "      'ffmpeg',\n",
        "      '-y',\n",
        "      '-i', video_file_path,\n",
        "      '-i', audio_file_path,\n",
        "      '-c:v', 'copy', # Copier la vidéo\n",
        "      '-c:a', 'aac', # Encoder l'audio en AAC\n",
        "      '-map', '0:v:0',\n",
        "      '-map', '1:a:0',\n",
        "      output_file_path\n",
        "    ]\n",
        "\n",
        "    # Exécuter la commande FFmpeg et capturer la sortie d'erreur\n",
        "    process = subprocess.run(ffmpeg_command, stderr=subprocess.PIPE)\n",
        "\n",
        "    # Afficher la sortie d'erreur\n",
        "    if process.returncode != 0:\n",
        "        print(f\"Erreur FFmpeg: {process.stderr.decode()}\")\n",
        "    else:\n",
        "        print(\"Vidéo avec audio créée\")\n",
        "    #output_file_path = video_file_path\n",
        "\n",
        "\n",
        "    return output_file_path\n",
        "\n",
        "\n",
        "\n",
        "def get_video_duration(video_file):\n",
        "  #video_file=\"/content/ren.mp4\"\n",
        "  # Charger le clip vidéo\n",
        "  video = VideoFileClip(video_file)\n",
        "\n",
        "  # Obtenir la durée totale en secondes\n",
        "  duree_totale = video.duration\n",
        "\n",
        "  # Convertir la durée totale en millisecondes\n",
        "\n",
        "  return duree_totale * 1000\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def dubbing_subtitle_video(input_file_video, source, target):\n",
        "  #extraire l'audio\n",
        "  output_file_audio = convert_to_wav(input_file_video)\n",
        "  file_audio_not_silence = remove_silence_from_end(output_file_audio)\n",
        "  result, complet_chain = transcription_yoruba(file_audio_not_silence, source)\n",
        "  list_sentences = translate_gpt_phrase(source, target, complet_chain)\n",
        "  raw_subtitles = generate_subtitles_raw(result, list_sentences)\n",
        "  srt_file = write_srt_file(raw_subtitles)\n",
        "  segments, start_times = generate_audio_segment(result, list_sentences,target)\n",
        "  video_duration = get_video_duration(input_file_video)\n",
        "  merge_audios = merge_audio_segments(segments, start_times, video_duration, \"merge_audio_segments\")\n",
        "  original_video_without_sound = delete_voice_video(input_file_video)\n",
        "  video_dumbbing = merge_audio_video(original_video_without_sound, merge_audios)\n",
        "  video_dumbbing_subtitle = integrer_sous_titres(video_dumbbing, srt_file, target, target)\n",
        "\n",
        "  return video_dumbbing_subtitle\n",
        "\n",
        "\n",
        "\n",
        "def write_srt_file(raw_subtitles, output_file_path ='./subtitles.srt'):\n",
        "\n",
        "  \"\"\"\n",
        "  Ecrire les sous titre dans un fichier srt\n",
        "  input : raw_subtitles, output_file_path (path/file.srt)\n",
        "\n",
        "  \"\"\"\n",
        "  #output_file_path='./subtitles.srt'\n",
        "  # Vérifier si le fichier existe déjà\n",
        "  # Ouvrir le fichier SRT en mode écriture\n",
        "  with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "      # Initialiser le compteur de séquences de sous-titres\n",
        "      sequence_number = 1\n",
        "\n",
        "      # Parcourir la liste de données\n",
        "      for item in raw_subtitles:\n",
        "          # Formater le temps de début et de fin\n",
        "          start_time, end_time = item['time'].split(' --> ')\n",
        "\n",
        "          # Écrire la séquence de sous-titres dans le fichier\n",
        "          file.write(f\"{sequence_number}\\n\")\n",
        "          file.write(f\"{start_time} --> {end_time}\\n\")\n",
        "          file.write(f\"{item['text']}\\n\\n\")\n",
        "\n",
        "          # Incrémenter le compteur de séquences de sous-titres\n",
        "          sequence_number += 1\n",
        "\n",
        "  return output_file_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU-mxGKBckpg"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQBc_OEAXrj7"
      },
      "outputs": [],
      "source": [
        "def dubbing_subtitle_video(input_file_video, source, target, langue_sous_titres):\n",
        "  '''\n",
        "  input_file_video = '/content/video_file.mp4'\n",
        "  source = ['fr', 'yo', 'en']\"\n",
        "  target = ['fr', 'yo', 'en']\n",
        "  '''\n",
        "  output_file_audio = convert_to_wav(input_file_video)\n",
        "  file_audio_not_silence = remove_silence_from_end(output_file_audio)\n",
        "  result, complet_chain = transcription_yoruba(file_audio_not_silence, source)\n",
        "  list_sentences = translate_gpt_phrase(source, target, complet_chain)\n",
        "  raw_subtitles = generate_subtitles_raw(result, list_sentences)\n",
        "  srt_file = write_srt_file(raw_subtitles)\n",
        "  segments, start_times = generate_audio_segment(result, list_sentences,target)\n",
        "  video_duration = get_video_duration(input_file_video)\n",
        "  merge_audios = merge_audio_segments(segments, start_times, video_duration, \"merge_audio_segments\")\n",
        "  original_video_without_sound = delete_voice_video(input_file_video)\n",
        "  video_dumbbing = merge_audio_video(original_video_without_sound, merge_audios)\n",
        "  video_dumbbing_subtitle = integrer_sous_titres(video_dumbbing, srt_file, target_mp4, langue_sous_titres)\n",
        "\n",
        "  return video_dumbbing_subtitle\n",
        "\n",
        "\n",
        "def speech_to_speech(input_file_audio, source, target):\n",
        "  \"\"\"\n",
        "\n",
        "  source = ['fr', 'yo', 'en', 'fon']\"\n",
        "  target = ['fr', 'yo', 'en', 'den', 'fon']\n",
        "  \"\"\"\n",
        "  output_file_audio = convert_to_wav(input_file_audio)\n",
        "  file_audio_not_silence = remove_silence_from_end(output_file_audio)\n",
        "  result, complet_chain = transcription_yoruba(file_audio_not_silence, source)\n",
        "  text_translate = translate_gpt(source, target, result)\n",
        "  return text_translate\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv8btVRXcqnp"
      },
      "source": [
        "# Mon TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjTc2DOcc3tH"
      },
      "outputs": [],
      "source": [
        "from transformers import VitsModel, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model = VitsModel.from_pretrained(\"facebook/mms-tts-yor\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mms-tts-yor\")\n",
        "\n",
        "text = \"\"\"Dídáàbòbò ara rẹ lọ́wọ́ àrùn kòrónà. Harun Kòrónà, kóvìdnàíntìn yìí ti tọ̀kalẹ̀ ní àporílẹ̀dè àgbàyé.Ọ̀nà tí ó dẹrajú látìfí dáábùgbára rẹ lọ́wọ́ àwọn kòrónà ni kí oran tí àwọn ìlànà ìdénà mẹ́tàwọ̀n yìí. Àkọkọ́, fọ́wọ́ rẹ méjèjì ló orìkóoré gbẹ̀lú òṣé àtí omi, tàbí kí oló Sanitárísà tí wọ́n pé lò pẹ̀lú òpọ̀ ìwọ́ tílé lé. Ré pé Sanitárísà náà gbẹ́ ní owó ọ̀rẹ dadá. Ekejì ni pé kí Oyèráfun fí fọwọ́ kan ojú imú tàbí ẹrun láìfọ̀ọ́wọ́ náà. Tí ó bá fọwọ́ kan ojú rí pé ó tú owó rẹ fọ̀.Ẹkẹta. Ló àwọn ìrojá pa kòkòrò tí wọ́n fọwọ́sí láti finú àwọn ibìtí ọwọ́ nkan lórékoré, rántí láti fọwó rẹ̀, yọ tónú ìbé Àtìlẹ́yìngbàtíóbá Nubẹ̀tán. Kí è sí àwọn kan wọ̀nyí tí ó bá wá níìtá? Bú ẹnú àtí mú rẹ nígbà pùbá tí obá sín tàbíwókọ́, pẹlúlélo tíjù tàbí ìgúnpà rẹ. Fọ́wọ́tàbí kí olosani táísàtágẹ́bí àtí ṣàlàyé́ lẹ́yìn tí obá sín tàbíwókọ́. Ré pé o yẹra fún ìfarakọọ̀ rẹpẹ̀ lọ́àwọ̀n ènìyàn tí a rùn kòrónà bántọ̀ni agbègbè rẹ̀. Ìgbésẹ̀ iṣẹ̀ pàtàkì ipí o báwàrinú iwó a tíṣa ìsàn. Àwọn nkọ́ mìíràn tí máa ńfinísínú iwó níjíjí àgbàlagbá tàbí ní a ìsán àgọ arápín tó ó súgàá tàbí àwọn ìsàn mìíràn tíó tíkóbá àjẹsárá inù. Dúrósílébí òbá ń ṣáìsàn àfíbí òbánálátìlọsílé ìwósàn nìkan. Yẹráfún ìfarakọ́ rẹpẹ̀ lọ́àwọ̀n ènìyàn tàbí ẹrókó nílé ìrán. Máa ṣe lọ ibi iṣẹ́ ilé ìwé tàbí ìpéjọpọ̀ àwọn ìnìyàn léta. Yẹra fún lílú ọkọ̀ ìgbóró tàbí ọkọ̀ Akéró. Oun tí ò yẹ kí ó mú ẹlọ lé ìwòsán níbí ìdìwọ̀n ìgbóná bájú méjìdìlógójì lọ, tàbí tí o bá wúkọ́lele, tàbí tí ò bálemi dadá. Bí àrùn kòrónàbántán ní agbègbé rẹ̀, yẹra fún òdélí lọ ńibí tí ọ̀pọ̀ ènìyàn báwà dúrósílé. Gbogbo wa, la gbọ́dọ̀ ṣiṣẹpọ̀ láti dènà ìtọ̀kálẹ̀ àrùnyé.Rántí láti fọ́wọ́ ọ̀rẹ́ pẹlò ọṣẹ a tí omí tàbí kí oló sánítáísá. Máṣéfọ́wọ́ kan ojú í mó tàbí ènùrẹ́ láìífọ́wọ́ bẹ́ẹ̀sí níkí oló ìrojápakó láti fìnàwọbí tí owónkàn lórèkórè. Jọ́, wáìmọ̀kú ìmọ̀lọ́dàwọ́n létó ìlérá agbègbé rẹ tàbí àjọ́ létó ilìrà agbàyé W.O.O. nítorí ọ̀pọ̀nkan tó ń fi ọ̀.......\"\"\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(**inputs).waveform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ux3gSuVvdCh3"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "import torch\n",
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Supposons que model et output sont déjà définis dans votre code\n",
        "# Assurez-vous que output est un tensor PyTorch contenant vos données audio\n",
        "\n",
        "# Convertissez le tensor PyTorch en un tableau numpy\n",
        "output_np = output.numpy()\n",
        "\n",
        "# Transposez le tableau si nécessaire pour avoir la forme (num_samples, num_channels)\n",
        "output_np = output_np.T\n",
        "\n",
        "# Normalisez les données pour qu'elles soient dans la plage [-32768, +32767] pour int16\n",
        "output_np = (output_np * 32767).astype(np.int16)\n",
        "\n",
        "# Écrivez le fichier WAV\n",
        "scipy.io.wavfile.write(\"/content/technopp.wav\", rate=16000, data=output_np)\n",
        "\n",
        "\n",
        "# Chargez le fichier WAV\n",
        "sound = AudioSegment.from_wav(\"/content/technopp.wav\")\n",
        "\n",
        "# Exportez le fichier en MP3\n",
        "sound.export(\"/content/technopp.mp3\", format=\"mp3\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
